{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67d2939a-6466-41b0-9cd0-76e751fe44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "import re\n",
    "import time\n",
    "from datetime import date, datetime,time\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab941ca-7566-4444-9345-cf727e7f46d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "india='https://news.google.com/topics/CAAqJQgKIh9DQkFTRVFvSUwyMHZNRE55YXpBU0JXVnVMVWRDS0FBUAE?hl=en-IN&gl=IN&ceid=IN%3Aen'\n",
    "world='https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRGx1YlY4U0JXVnVMVWRDR2dKSlRpZ0FQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen'\n",
    "business='https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRGx6TVdZU0JXVnVMVWRDR2dKSlRpZ0FQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen'\n",
    "technology='https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRGRqTVhZU0JXVnVMVWRDR2dKSlRpZ0FQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen'\n",
    "entertainment='https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNREpxYW5RU0JXVnVMVWRDR2dKSlRpZ0FQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen'\n",
    "sports='https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRFp1ZEdvU0JXVnVMVWRDR2dKSlRpZ0FQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen'\n",
    "science='https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRFp0Y1RjU0JXVnVMVWRDR2dKSlRpZ0FQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen'\n",
    "health='https://news.google.com/topics/CAAqJQgKIh9DQkFTRVFvSUwyMHZNR3QwTlRFU0JXVnVMVWRDS0FBUAE?hl=en-IN&gl=IN&ceid=IN%3Aen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afe3a625-aeca-4e0f-aa43-8784eef9bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_respone(url,category):\n",
    "    \"\"\"\n",
    "    The fuction take google news url as input and for several articles on the page it scrapes the article content and stores \n",
    "    as text files in respective folder\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.get(url) ### using request module to take and store response of the url\n",
    "    soup = BeautifulSoup(response.text, 'html.parser') ### parsing the response and souping out html text \n",
    "\n",
    "    \"\"\"\n",
    "    For each article scraping out the article content and appending to the list \n",
    "\n",
    "    \"\"\"\n",
    "    article_link_list=[]\n",
    "    domain='https://news.google.com'\n",
    "    for n in soup.find_all('article'):\n",
    "        for x in n.find_all('a'): \n",
    "            article_link_list.append (domain+x.get('href')[1:])\n",
    "    t=time.time()\n",
    "    print(len(article_link_list))\n",
    "\n",
    "    data_to_insert = {}\n",
    "    data_to_insert['Category'] = category  # Assigning a category\n",
    "    # List to store news details\n",
    "    news_details = []\n",
    "    \n",
    "    # Iterate over each link in the list of article links\n",
    "    for link in article_link_list:\n",
    "        try:\n",
    "            # Fetch the HTML content of the article link and create a BeautifulSoup object\n",
    "            soup = BeautifulSoup(requests.get(link).text, 'html.parser')\n",
    "            \n",
    "            try:\n",
    "                # Initialize an empty string to store the article text\n",
    "                article_text = ''\n",
    "                \n",
    "                # Extract text from all <p> tags and append to the article_text\n",
    "                for p_text in soup.find_all('p'):\n",
    "                    article_text = article_text + p_text.text\n",
    "                \n",
    "                # Create a dictionary to store news details\n",
    "                news = {\n",
    "                    'title': soup.find_all('h1')[0].text,  # Extract title from the first <h1> tag\n",
    "                    'article': article_text,  # Assign article text\n",
    "                    'url': link,  # Assign article URL\n",
    "                    'timestamp': datetime.today()  # Assign current date as timestamp\n",
    "                }\n",
    "                \n",
    "                # Append news details to the list of news\n",
    "                news_details.append(news)\n",
    "            \n",
    "            except Exception as e:\n",
    "                # Handle exceptions if any\n",
    "                pass\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Handle exceptions if any\n",
    "            pass\n",
    "    data_to_insert['news']=news_details\n",
    "    print(time.time()-t) \n",
    "    return data_to_insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a24f4bd-31ab-4940-a7cf-7eff21558730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545\n",
      "733.9230215549469\n",
      "523\n",
      "727.6835308074951\n",
      "534\n",
      "711.3960950374603\n",
      "496\n",
      "635.517110824585\n",
      "517\n",
      "768.3167524337769\n",
      "519\n",
      "662.4009916782379\n",
      "247\n",
      "365.3033711910248\n",
      "412\n",
      "572.9857876300812\n"
     ]
    }
   ],
   "source": [
    "final_collection=[]\n",
    "a0=fetch_respone(india,'india')\n",
    "a1=fetch_respone(world,'world')\n",
    "a=fetch_respone(business,'business')\n",
    "b=fetch_respone(technology,'technology')\n",
    "c=fetch_respone(entertainment,'entertainment')\n",
    "d=fetch_respone(sports,'sports')\n",
    "e=fetch_respone(science,'science')\n",
    "f=fetch_respone(health,'health')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94fc7ac7-518b-4fe2-a71e-31653926ef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_collection.append(a0)\n",
    "final_collection.append(a1)\n",
    "final_collection.append(a)\n",
    "final_collection.append(b)\n",
    "final_collection.append(c) \n",
    "final_collection.append(d)\n",
    "final_collection.append(e)\n",
    "final_collection.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc44dbea-a7ae-4df2-b9d3-d82ce3b577c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = requests.get(india) ### using request module to take and store response of the url\n",
    "# soup = BeautifulSoup(response.text, 'html.parser') ### parsing the response and souping out html text \n",
    "# article_link_list=[]\n",
    "# domain='https://news.google.com'\n",
    "# for n in soup.find_all('article'):\n",
    "#     for x in n.find_all('a'): \n",
    "#         article_link_list.append (domain+x.get('href')[1:])\n",
    "# t=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e015586-5bda-4da4-aaa5-d70fc3f949ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "def wrtite_data_to_mongo(data):\n",
    "    # Connect to MongoDB\n",
    "    client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"News_articles\"]\n",
    "    collection = db[\"daily_news\"]    \n",
    "    # Insert data into the collection\n",
    "    collection.insert_many(data)\n",
    "    \n",
    "    # Print success message\n",
    "    print(\"Data inserted successfully!\")\n",
    "wrtite_data_to_mongo(final_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f32bda-c4a3-46c1-8d2d-f877aec80913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
